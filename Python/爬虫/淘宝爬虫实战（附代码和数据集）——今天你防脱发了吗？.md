# 淘宝爬虫实战（附代码和数据集）——今天你防脱发了吗？

[![吹牛顿](https://pic1.zhimg.com/f12e3d5c1ab81390e40041e165a1beef_xs.jpg)](https://www.zhihu.com/people/bie-he-he-58)

[吹牛顿](https://www.zhihu.com/people/bie-he-he-58)

公众号：数据不吹牛，更多数分有趣案例和代码等你来撩

关注他

109 人赞同了该文章

>   本文通过淘宝“防脱发洗发水”爬取和分析，来提供爬取海量淘宝商品信息的思路，除了基础爬虫外，还应该思考拿到类似的商品数据之后如何清洗，以及作为一个分析者可以从什么维度去分析。


完整代码放在文末，如果需要一手数据集（4400条新鲜商品数据）实践的同学**关注公众号“数据不吹牛”**，点击实战数据即可。



其实，这篇文章灵感源自一个赌局：

程序员朋友小A又在和小Z抱怨脱发问题。

小A：“以这样的掉发速度，我的发际线1年后将退化到后脑勺”。

“我听到身边80%的人都在抱怨自己的脱发问题”小Z摸了摸自己的发际线心如止水。

小A：”有危机就有商机，防脱发洗发水最近真的是卖爆了，特别在线上，绝对占了洗发水整个行业的半壁江山以上！”

小Z总能GET到奇怪的点：“你这样的说法不严谨，我觉得没有50%”。

小A被奇葩的问题点给气到了：“WOC！你的点怎么那么怪！不然咱们打个赌好吗，我赌防脱发占了50%以上，谁输谁是孙子（zei）！”

只用了3分钟，小Z就拟定好分析思路，并得到了小A的认可：

**以淘宝入手，爬到最近30天洗发水关键词的销售情况，再筛选出防脱发洗发水，看一看占比多少。**（顺便还可以分析分析其他的数据）

说干就干，打开淘宝，搜索“洗发水”，出来的是自然排序的结果（综合了销量、价格、搜索权重等等），但我们想要相关商品按销量来排序，点击“按销量排序”。



![img](https://pic3.zhimg.com/80/v2-faf070243a0de7d036966044aad01d06_720w.png)





**一、数据爬取**

**PART1 观察并定位数据：**



![img](https://pic2.zhimg.com/80/v2-5a95184dec1a54010a1bdd4f1b11465d_720w.jpg)



我们想要哪些数据呢？

商品的价格、月收货（销售）人数、产品名称、店铺名称、店铺地址这几个比较直观的字段我们爬取哪几个呢？

小孩子才做选择，成年人必须全要！

虽然现在很多网址都是动态加载，需要审查元素来找相关地址，但我们在找之前，养成“先右键，选择查看源代码，看一看想要的数据有没有在静态网页”的习惯是极好的。

结果淘宝诚不欺我，所有我们想要的数据，都在源代码中，也就是说，我们用PYTHON直接访问浏览器中的网址就可以得到目标数据。

认真看看源代码，找到更准确的定位



![img](https://pic1.zhimg.com/80/v2-57eadc26b178a43dec884b34279b45d4_720w.jpg)



所有想要的数据都在一个类JSON（可以先理解为字典）的字符串中，而前面还有几十行杂乱无章的字符，很乱，但不要紧，数据在总有办法找到他们的。



**PART2 请求尝试**

这里引用上一篇文章的一段话来比喻PYTHON访问前的伪装

“你住在高档小区，小P这个坏小伙想伪装你进去做不可描述的事情。

他知道，门卫会根据身份象征来模糊判断是否是小区业主，所以小P先租了一套上档次的衣服和一辆称得上身份的豪车（可以理解为伪装headers），果然混过了门卫。但是呢，小P进进出出太频繁，而且每次停车区域都不一样，引起了门卫的严重怀疑，在一个星期后，门卫升级检验系统，通过人脸识别来验证，小P被拒绝在外，但很快，小P就通过毁容级别的化妆术（伪装cookies），完全伪装成你，竟然混过了人脸识别系统，随意出入，为所欲为。”

导入相关的PYTHON库



![img](https://pic4.zhimg.com/80/v2-3fc9109a751f02147b4aa658cb65f023_720w.png)



养成先修改headers的好习惯再访问：



![img](https://pic4.zhimg.com/80/v2-e756bbbd67dd957a36a106bc394a0fbb_720w.png)



看看状态码（200表示正常访问）：



![img](https://pic2.zhimg.com/80/v2-722865983915a354f64c57d70959cc95_720w.jpg)



目前来说，还算正常，但堂堂的淘宝这么简单的一个伪装就可以爬了？？？不科学！！不过先继续吧，精确定位到我们需要的数据字段。

上一步，我们发现所有的数据都在一个类JSON的字符串中，理应先精确定位他首尾的大括号（｛｝），尝试用JSON来高效解析。

首：



![img](https://pic2.zhimg.com/80/v2-af12e0c79bf32dee434e40fc6eab9939_720w.png)



尾：



![img](https://pic3.zhimg.com/80/v2-cb99f39fb24167a28f4f24cfe4ab9d92_720w.png)



通过严密的排查（同学们这一步真的需要耐心去找），我们发现所有目标数据都被包裹在以pageName开头，shopcardOff的字符中，如果能够完整截取这个大括号和里面的内容，就可以解析了：



![img](https://pic4.zhimg.com/80/v2-5ee354ce92ea62ef565f011633a6c697_720w.jpg)



结果。。。报错啊报错。。。



![img](https://pic3.zhimg.com/80/v2-96f38f61593b80bad7681c32a4467786_720w.png)



我们没有通过字符串定位拿到想要的数据，通过系统排查，发现问题出在访问，第一次访问虽然状态码是200，但并没有返回源代码看到的数据，喏：



![img](https://pic4.zhimg.com/80/v2-f2960a98512f06d77bd5e81530e18e77_720w.png)



到这里，是时候祭出万能的cookies了，操作方式，右键——审查元素——刷新网页——按照下面红框点选：



![img](https://pic2.zhimg.com/80/v2-6d6bf0b9360b0f9aab22e1e7ef5e67b9_720w.jpg)



代码中进行伪装：



![img](https://pic4.zhimg.com/80/v2-94df5a22fd1008b2fa5c7ab2b98bbdd3_720w.png)



再次按照刚才的步骤来定位和解析数据：



![img](https://pic1.zhimg.com/80/v2-1e8a96bb2f9c04e66b6e15502ac7b808_720w.jpg)



一样的操作，没有报错，看来大功告半成！



**PART3 精确定位目标数据：**

经过前面两步的铺垫，我们已经拿到了目标数据并解析成JSON格式，现在直接可以按照访问字典的方式来精确定位数据，非常暴力（至于内部的层级结构，需要大家耐心细致的自我寻找规律）：



![img](https://pic1.zhimg.com/80/v2-eb12715af63651c8c99b12b9af67ea78_720w.jpg)





**PART4 循环爬取：**

循环爬取的关键就在于找到网址规律，构建多个网页，用上面的代码来循环访问。

我们在网页上点击下一页，再下一页，再下下一页，很容易发现，网站变化规律的核心就是最后面

s的值，第一页是0，第二页是44，第三页是88，SO EASY~

构造一个自定义爬取页数的函数，只需要输入基础网址和要爬取的页数，要多灵活有多灵活：



![img](https://pic1.zhimg.com/80/v2-1b9c66afd7e170cf3c0dfe325d5c36f8_720w.jpg)



接上一步的访问获取数据操作进行逐页访问，即实现了多页面爬取，部分结果预览如下：



![img](https://pic1.zhimg.com/80/v2-e13611a6d78fde8f1079d9936d603a8c_720w.jpg)



至此，商品标题，价格，店铺名称，店铺地址，收货人数，商品的URL全部拿下，基于“防脱发洗发水”的基本数据爬取宣告完成。（完整代码在文章最后）



------



**二、数据清洗**

清洗之前，最好先明确分析的目的，小Z最核心的诉求是要知道脱发洗发水销售占整个洗发水大盘的比重，其次，想要进行一些其他分析，比如渠道（旗舰店、专营店、猫超等等分别占比）分布。

**1、数字相关字段规整：**



![img](https://pic3.zhimg.com/80/v2-5c80f59d2040b706570a8b20c703ba4e_720w.jpg)



爬取数据非常规整，并没有缺失数据。

价格也是OK的，付款人数由于包含“人收货”这个后缀，需要规整为数字格式，一行代码就OK：



![img](https://pic3.zhimg.com/80/v2-8f6ee3da5cc8c56f536049d54fd2d0de_720w.jpg)



**2、标注出脱发相关的产品：**

很明显，如果主打甚至仅仅包含防脱发功效的产品几乎都会在标题注明“脱发”字样（防字其实不用加），我们需要插入一个辅助列，根据“产品标题”来判断是不是防脱发洗发水。

PYTHON的pandas做起来是在是太高效了，还是一行代码：



![img](https://pic2.zhimg.com/80/v2-f401da3fdd5ed1642ace9f21f4b6f339_720w.png)



注：等于-1表示在标题中没有找到“脱发”字样



![img](https://pic1.zhimg.com/80/v2-365ad0cb36125d746efd0c69b6fcedf4_720w.jpg)



“是否包含脱发字样”结果为TRUE则包含，FALSE则不包含。



**3、引入一个销售指标**

目前拿到的数字相关数据是“价格”、“收货人数”，用“价格” * “收货人数”引入一个“收货额”来衡量销售情况，依然是一行代码：



![img](https://pic4.zhimg.com/80/v2-db8170eab16f4489860b7bcb1fcc40cf_720w.png)





**4、区分店铺类别：**

大家都有多年购买经验，对于淘宝店铺分类其实不陌生，不外乎是“旗舰店”、“专卖店”、“专营店”、“天猫超市”、“C店”（其他淘宝店铺），这里需要对店铺关键字进行检索分类，先定义一个判断函数：



![img](https://pic4.zhimg.com/80/v2-0025da6c8c84446e643a35dc3c235b43_720w.jpg)



然后，life is short,and i use Python~

亦然是一行代码搞定：



![img](https://pic1.zhimg.com/80/v2-39b9673ba977790862a5910513011208_720w.jpg)



数据清洗基本完成。



------



**三、数据分析：**

**1、核心目标：**



![img](https://pic4.zhimg.com/80/v2-ee6f7169b964213187482a05eada1e27_720w.jpg)



言归正传，目前“洗发水”类目体量巨大，(近30天)收货额达到了1.49亿元，其中防脱发洗发水以5.43%的数量占比实现1118.04万销售额，占比7.50%，离半壁江山相差甚远，赌局胜负已定，恭喜小Z喜提孩子。

“孩子，在数据面前可不能吹牛啊”小Z看着小A涨红了的脸语重心长道。



**2、价格分布**

价格深度探究应该结合产品的数量、规格等特征，这里只是给到一个简单的思路抛砖引玉：



![img](https://pic3.zhimg.com/80/v2-e2ef5df8b6e0a4f9f6a294baf23f47e6_720w.jpg)



两款产品呈现出不同的分布形态，防脱发洗发水在价格上显得些许傲娇，产品在50-100元的价格段数量最多（占比51.88%），其次是0-50元的平价款。

其他洗发水则随着价格升高而数量减少，0-50元的产品占比最高，紧随其后的是50-100元的产品。



**3、渠道分布：**



![img](https://pic1.zhimg.com/80/v2-a793eb4beb1636595bfa00796a45f3d8_720w.jpg)



不同类型洗发水（防脱发与非防脱发）渠道策略有明显的差异（肯定跟品牌战略有关），其他洗发水渠道分布相对均衡，以“旗舰店”的41%为主，“天猫超市”为辅（29%），“C店”和“专卖店”分一小杯羹。

防脱发洗发水则高举旗舰店利剑（占比高达77%+），其次则是各类C店（11%），而在其他洗发水渠道表现优异的猫超在这里折戟，仅占比3%。

**看来，防脱发类功能产品高销售背后离不开品牌的背书支撑。（一般品牌才会开设旗舰店）**



最后，附上完整的项目代码：

```text
#构造循环爬取的函数
def format_url(base_url,num):    
    urls = []
    for i in range(0,num * 44,44):
        urls.append(base_url[:-1] + str(i))
    return urls

#解析和爬取单个网页
def parse_page(url,cookies,headers):
    result = pd.DataFrame()
    html = requests.get(url,headers = headers,cookies = cookies)
    bs = html.text
    #获取头部索引地址
    start = bs.find('g_page_config = ') + len('g_page_config = ')
    #获取尾部索引地址
    end = bs.find('"shopcardOff":true}') + len('"shopcardOff":true}')
    js = json.loads(bs[start:end + 1])

    #所有数据都在这个auctions中
    for i in js['mods']['itemlist']['data']['auctions']:
        #产品标题
        product = i['raw_title'] 
        #店铺名称
        market = i['nick']
        #店铺地址
        place = i['item_loc']
        #价格
        price = i['view_price']
        #收货人数
        sales = i['view_sales']
        url = 'https:' + i['detail_url']
        r = pd.DataFrame({'店铺':[market],'店铺地址':[place],'价格':[price],
                     '收货人数':[sales],'网址':[url],'产品标题':[product]})
        result = pd.concat([result,r])
    time.sleep(5.20)
    return result

#汇总
def main():
    #爬取的基准网页（s = 0）
    base_url = 'https://s.taobao.com/search?q=%E6%B4%97%E5%8F%91%E6%B0%B4&imgfile=&commend=all&ssid=s5-e&search_type=item&sourceId=tb.index&spm=a21bo.2017.201856-taobao-item.1&ie=utf8&initiative_id=tbindexz_20170306&sort=sale-desc&bcoffset=0&p4ppushleft=%2C44&s=0'
    #定义好headers和cookies
    cookies = {'cookie':'输入自己的COOKIES'}
    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}

    #设置好存储结果的变量
    final_result = pd.DataFrame()

    #循环爬取5页
    for url in format_url(base_url,5):
        final_result = pd.concat([final_result,parse_page(url,cookies = cookies,headers = headers)])
    return final_result
    
if __name__ == "__main__":
    final_result = main()
```

![img](https://pic4.zhimg.com/80/v2-38bb1a95e0d6c741a68edee7e431edd7_720w.jpg)